<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[神经网络算法代码实现]]></title>
    <url>%2F2019%2F10%2F12%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[NeraulNetwork.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244import numpy as np# 双曲正切函数def tanh(x): return np.tanh(x)# 双曲正切函数导数def tanh_derivative(x): return 1 - np.tanh(x) * np.tanh(x)# sigmoid函数def sigmoid(x): return 1 / (1 + np.exp(-x))# sigmoid函数导数def sigmoid_derivative(x): return sigmoid(x) * (1 - sigmoid(x))# 神经网络实现类class NeuralNetwork(): # layers:神经网络 # activation:激励函数 # learning_rate 学习率 # loss_threshold:损失阀值 # epoch:最大训练次数 def __init__(self, layers=[], activation=&quot;sigmoid&quot;, learning_rate=0.1, epoch=1000, loss_threshold=0.01): if activation == &quot;sigmoid&quot;: self.activation = sigmoid self.activation_derivative = sigmoid_derivative elif activation == &quot;tanh&quot;: self.activation = tanh self.activation_derivative = tanh_derivative else: self.activation = sigmoid self.activation_derivative = sigmoid_derivative self.layers = layers self.init_weights(layers) self.init_bias(layers) self.init_nodes() self.init_errors() self.learning_rate = learning_rate self.epoch = epoch self.loss_threshold = loss_threshold # 校验二维数组 def valiad_two_array(self, data): if isinstance(data, list) and len(data) &gt; 0: if isinstance(data[0], list) == False or len(data[0]) == 0: raise RuntimeError(&quot;参数错误,请传一个不为空的二维数组&quot;) else: raise RuntimeError(&quot;参数错误,请传一个不为空的二维数组&quot;) # 校验一维数组 def valid_one_array(self, data): if isinstance(data, list) == False or len(data) == 0: raise RuntimeError(&quot;参数错误,请传入一个不为空的一维数组&quot;) # 初始化权重 def init_weights(self, layers): self.weights = [] for i in range(1, len(layers)): self.weights.append(np.random.random((layers[i - 1], layers[i]))) # 初始化偏向 def init_bias(self, layers): self.bias = [] for i in range(1, len(layers)): self.bias.append(np.random.random((layers[i], 1))) # 训练模型 def fit(self, data): self.valiad_two_array(data) self.counter = 0 for i in range(len(data)): self.training_data(data[i], i) # 预测数据 def predict(self, data): self.valiad_two_array(data) counter = 0 for one_data in data: self.forward_propagation(one_data) predict = self.nodes[len(self.layers) - 1] for i in range(len(predict)): predict[i] = self.handle_by_threshold(predict[i]) print(&quot;predict[&#123;&#125;] = &#123;&#125; &quot;.format(counter, predict)) counter += 1 # 根据阀值处理数据 def handle_by_threshold(self, data): if data &gt;= 0.5: return 1 else: return 0 # 一次训练流程 def training_data(self, one_data, number): self.loss = 1 one_training_counter = 0 while self.loss &gt; self.loss_threshold and one_training_counter &lt; self.epoch: self.counter += 1 one_training_counter += 1 self.forward_propagation(one_data) self.back_propagation_error(one_data) self.back_propagation_update_weights() self.back_propagation_update_bias() # print(&quot;总次数&#123;&#125;,第&#123;&#125;行数据,当前次数:&#123;&#125;,\n&#123;&#125;&quot;. # format(self.counter, number, one_training_counter, self.get_obj_info())) # 获取对象信息 def get_obj_info(self): info = &quot;\n\n weights: &quot; + str(self.weights) \ + &quot;\n\n bais: &quot; + str(self.bias) \ + &quot;\n\n nodes: &quot; + str(self.nodes) \ + &quot;\n\n errors: &quot; + str(self.errors) \ + &quot;\n\n loss: &quot; + str(self.loss) return info # 输出层错误计算 # out:经过激励函数计算后的结果 # predict:原始预测的结果 def calculate_out_layer_error(self, out, predict): return out * (1 - out) * (predict - out) # 隐藏层错误计算 # out:经过激励函数计算后的结果 # errors:下一层所有节点的损失合计 def calculate_hidden_layer_error(self, out, errors): return out * (1 - out) * errors # 前向传播,递归得到每一个节点的值 # one_row_data:一行数据 # counter: 计数器 def forward_propagation(self, one_row_data, counter=0): if counter == 0: input = self.get_input(one_row_data) self.input = input for i in range(len(self.input)): self.nodes[0][i] = self.input[i] counter += 1 if counter == len(self.layers): return current_nodes = self.nodes[counter] pre_nodes = self.nodes[counter - 1] for i in range(len(current_nodes)): current_value = 0 for j in range(len(pre_nodes)): pre_node = pre_nodes[j] pre_weights = self.weights[counter - 1][j][i] current_value += pre_node * pre_weights current_bias = self.bias[counter - 1][i][0] current_value = (current_value + current_bias)[0] current_node = self.activation(current_value) current_nodes[i] = current_node self.forward_propagation(one_row_data, counter + 1) # 得到特征值 def get_input(self, one_row_data): return one_row_data[:self.layers[0]] # 根据特征值真实结果 def get_out(self, one_row_data): return one_row_data[self.layers[0]:] # 后向传播,得到误差 def back_propagation_error(self, one_row_data, counter=-1): if counter == -1: # 第一次进入方法，初始化 counter = len(self.layers) - 1 out = self.get_out(one_row_data) self.out = out if counter == 0: # 遍历集合(第一层输入层不计算损失) return current_nodes = self.nodes[counter] if counter == len(self.layers) - 1: # 输出层损失计算 loss = 0 for i in range(len(current_nodes)): current_node = current_nodes[i] predict = self.out[i] error_value = self.calculate_out_layer_error(current_node, predict) self.errors[counter][i] = error_value loss += pow(predict - current_node, 2) self.loss = loss else: # 隐藏层损失计算 next_errors = self.errors[counter + 1] for i in range(len(current_nodes)): current_node = current_nodes[i] errors = 0 for j in range(len(next_errors)): error = next_errors[j] weight = self.weights[counter][i] errors += error * weight error_value = self.calculate_hidden_layer_error(current_node, errors) self.errors[counter][i] = error_value self.back_propagation_error(one_row_data, counter - 1) # 后向传播,更新权重 def back_propagation_update_weights(self): for i in reversed(range(len(self.layers) - 1)): current_nodes = self.nodes[i] errors = self.errors[i + 1] for j in range(len(current_nodes)): for m in range(len(errors)): error = errors[m] current_node = current_nodes[j] weight = self.weights[i][j][m] weight_delta = self.learning_rate * error * current_node update_weight = weight + weight_delta self.weights[i][j][m] = update_weight # 后向传播,更新偏向 def back_propagation_update_bias(self): for i in reversed(range(len(self.layers) - 1)): bias = self.bias[i] for j in range(len(bias)): error = self.errors[i + 1][j] bias_delta = self.learning_rate * error bias[j] += bias_delta # 设置权重 def set_weights(self, weights): self.weights = weights # 设置偏向 def set_bias(self, bias): self.bias = bias # 初始化所有节点（节点值设置为一个随机数) def init_nodes(self): self.nodes = [] for i in range(len(self.layers)): self.nodes.append(np.random.random((self.layers[i], 1))) # 初始化所有节点损失值(损失值设置为一个随机数) def init_errors(self): self.errors = [] for i in range(len(self.layers)): self.errors.append(np.random.random((self.layers[i], 1)))]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>ANN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cases of the Artificial Neural Networks]]></title>
    <url>%2F2019%2F10%2F08%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;人工神经网络（Artificial Neural Networks,简写ANN）是以一种模仿动物神经网络特征，进行分布式并行神经处理的算法数学模型。由大量的节点（或称神经元）组成。每个节点对应不同的输出函数，称为激活函数（activation function）,节点与节点之间对应不同的加权值，称之为权重。 前向传播 &emsp;&emsp;pass 反向传播&emsp;&emsp;反向传播（Backword Propagation）是“误差反向传播”的简称，是一种与梯度下降算法结合使用的，用来训练人工神经网络的重要算法。该算法通过计算损失函数对神经网络中各个权值的偏导数来最小化损失函数。是神经网络执行梯度下降算法的主要算法。该算法先前向计算出各个节点的状态以及输出值，然后再以反向计算损失函数对各个参数的偏导数。反向传播算法主要由激励传播和权值更新两个过程反复迭代，直到网络对输入的响应达到目标要求为止。本文将按照单一训练集介绍反向传播算法中重要公式的推导，后续扩展到m维训练集。&emsp;&emsp;单一训练集时符号说明如下： 参数矩阵的维度&emsp;&emsp;pass 非线性激活函数的对比分析&emsp;&emsp;pass]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Neural Network</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
</search>
