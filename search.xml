<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于Attention机制的Encoder-Decoder框架实现机器翻译]]></title>
    <url>%2F2020%2F05%2F07%2Fseq2seq-attention%2F</url>
    <content type="text"><![CDATA[循环神经网络(Recurrent Neural Networks) 为什么不是多层感知机(全连接前馈网络)？ 多层感知机不擅长或者说不能处理可变长序列 多层感知机只能处理输入数据和输出数据具有固定长度的序列。即使可将每条数据都填充(pad)至最大长度，用以训练多层感知机，但似乎仍然不是一个很好的模型。 多层感知机不能进行参数共享 如果在每个时间点都具有相同的参数，不但不能泛化到训练时没有见过的序列长度的数据，也不能在时间上共享不同序列长度和不同位置的统计强度。比如在一句话的两个不同位置出现了两次单词-Harry，多层感知机需要对每个Harry都进行学习。 为什么是循环神经网络？ 不同时间步内的参数共享 不需要学习每个位置的规则 1. Seq2Seq模型&emsp;&emsp;Seq2Seq模型用来解决将可变长的输入序列映射到可变长的输出序列。这在许多场景中都有应用，如语音识别、机器翻译或问答等，其中训练集的输入序列和输出序列的长度通常不相同（虽然它们的长度可能相关）。 1.1 Encoder-Decoder架构1.1.1 架构基本原理及组成&emsp;&emsp;用于将可变长度序列映射到另一可变长度序列最简单的RNN架构是Encoder-Decoder架构。其基本思想是：编码器的每个时间步分别输入单词特征向量（采用one-hot编码或Embedding），并输出上下文向量C（通常是最终时间步的隐层状态或其简单函数），通过该向量来表征输入句子的信息。解码器在每个时间步使用上下文向量C，上一时间步输出以及上一时间步的隐层状态为输入进行预测。如下图所示为Encoder-Decoder架构示意图，其中上下文向量通常为固定长度向量。 编码器Encoder&emsp;&emsp;Encoder本身是一个RNN网络，RNN的单元可以是GRU，也可是LSTM。可以为单向,也可以为双向。对于长度为$T_x$输入序列${x_1, x_2, … x_{T_x}}$, 在时间步$t$, 将单词$x_t$的特征向量和上个时间步的隐藏状态$h_{t-1}$, Encoder的作用是将一个可变长度序列映射为固定大小的上下文向量C，用以表征输入序列的语意概要。其中:时间步$t$的隐层状态为:&emsp;&emsp;&emsp;&emsp;$h_t = f(x_t, h_{t-1})$上下文向量$C$为:&emsp;&emsp;&emsp;&emsp;$C = q(h_1, h_2, h_3, … , h_T)$ 解码器Decoder&emsp;&emsp;Decoder也是一个RNN网络，其工作原理如下：对于输出序列的时间步$t{‘}$, 解码器以上一时间步的隐层状态$h_{t{‘}-1}$以及上下文向量$C$为输入，输出$y_{t{‘}}$的条件概率，即$P(y_{t{‘}}|y_1, y_2, … , y_{t{‘}-1}, C)$。 训练模型&emsp;&emsp;根据极大似然估计，该问题为最优化问题，问题的目标函数为：&emsp;&emsp;$P(y_1, y_2, … , y_{t{‘}-1}|x_1, x_2, …, x_T)$&emsp;&emsp;$= \prod_{t{‘}=1}^{T{‘}}P(y_t|y_1, y_2, … , y_{t{‘}-1}, x_1, x_2, …, x_T)$&emsp;&emsp;$= \prod_{t{‘}=1}^{T{‘}}P(y_t|y_1, y_2, … , y_{t{‘}-1}, C)$&emsp;&emsp;该输出序列的损失函数为：&emsp;&emsp;$-logP(y_1, y_2, … , y_{t{‘}-1}|x_1, x_2, …, x_T)$&emsp;&emsp;$=\sum_{t{‘}=1}^{T{‘}}P(y_t|y_1, y_2, … , y_{t{‘}-1}, C)$ 1234567891011import tensorflow as tfimport matplotlib as mplimport matplotlib.pyplot as plt%matplotlib inlineimport numpy as npimport sklearnimport pandas as pdimport osimport sysimport timefrom tensorflow import keras 12345678910# 1. preprocessing data# 2. build model# 2.1 encoder# 2.2 attention# 2.3 decoder# 2.4 loss &amp; optimizer# 2.5 train# 3. evaluation# 3.1 given sentence, return translated results# 3.2 visualize results(attention) 123456789101112131415en_spa_file_path = './data_spa_en/spa.txt'import unicodedatadef unicode_to_ascii(s): '''西班牙语中的特殊字符是使用UNICODE表示的，需要转换成ASCII码。转换成ASCII后，vocabe_size=128 or 256''' return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')en_sentence = 'Then what?'sp_sentence = '¿Entonces qué?'print(unicode_to_ascii(en_sentence))print(unicode_to_ascii(sp_sentence)) Then what? ¿Entonces que?12345678910111213141516171819import redef preprocess_sentence(s): s = unicode_to_ascii(s.lower().strip()) # 标点符号前后加空格 s = re.sub(r"([?.!,¿])", r" \1 ", s) # 多余的空格变成一个空格 s = re.sub(r'[" "]+', " ", s) # 除了标点符号和字母外都是空格 s = re.sub(r'[^a-zA-Z?.!,¿]', " ", s) # 去掉前后空格 s = s.rstrip().strip() s = '&lt;start&gt; ' + s + ' &lt;end&gt;' return sprint(preprocess_sentence(en_sentence))print(preprocess_sentence(sp_sentence)) &lt;start&gt; then what ? &lt;end&gt; &lt;start&gt; ¿ entonces que ? &lt;end&gt;123456789101112def parse_data(filename): lines = open(filename, encoding='UTF-8').read().strip().split('\n') sentence_pairs = [line.split('\t') for line in lines] preprocessed_sentence_pairs = [(preprocess_sentence(en), preprocess_sentence(sp)) for [en, sp] in sentence_pairs] return zip(*preprocessed_sentence_pairs)en_dataset, sp_dataset = parse_data(en_spa_file_path)print(en_dataset[-1])print(sp_dataset[-1]) &lt;start&gt; if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . &lt;end&gt; &lt;start&gt; si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . &lt;end&gt;12345678910111213141516171819202122def tokenizer(lang): lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words=None, filters='', split=' ') lang_tokenizer.fit_on_texts(lang) tensor = lang_tokenizer.texts_to_sequences(lang) tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post') return tensor, lang_tokenizerinput_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])print(input_tensor.shape)output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])def max_length(tensor): return max(len(t) for t in tensor)max_length_input = max_length(input_tensor)max_length_output = max_length(output_tensor)print(max_length_input, max_length_output) (30000, 16) 16 111234from sklearn.model_selection import train_test_splitinput_train, input_eval, output_train, output_eval = train_test_split( input_tensor, output_tensor, test_size=0.2)len(input_train), len(input_eval), len(output_train), len(output_eval) (24000, 6000, 24000, 6000)123456789def convert(example, tokenizer): for t in example: if t != 0: print('%d --&gt; %s' % (t, tokenizer.index_word[t]))convert(input_train[0], input_tokenizer)print()convert(output_train[0], output_tokenizer) 1 --&gt; &lt;start&gt; 26 --&gt; yo 160 --&gt; tenia 239 --&gt; anos 20 --&gt; en 3 --&gt; . 2 --&gt; &lt;end&gt; 1 --&gt; &lt;start&gt; 4 --&gt; i 26 --&gt; was 33 --&gt; in 3 --&gt; . 2 --&gt; &lt;end&gt;12345678910111213def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle): dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor)) if shuffle: dataset = dataset.shuffle(30000) dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder=True) return datasetbatch_size = 64epochs = 20train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)eval_dataset = make_dataset(input_eval, output_eval, batch_size, 1, False) 12345for x, y in train_dataset.take(1): print(x.shape) print(y.shape) print(x) print(y) (64, 16) (64, 11) tf.Tensor( [[ 1 7824 13 ... 0 0 0] [ 1 6 11 ... 0 0 0] [ 1 6 14 ... 0 0 0] ... [ 1 137 497 ... 0 0 0] [ 1 12 597 ... 0 0 0] [ 1 16 7 ... 0 0 0]], shape=(64, 16), dtype=int32) tf.Tensor( [[ 1 116 126 13 465 3 2 0 0 0 0] [ 1 32 2077 20 7 2 0 0 0 0 0] [ 1 32 1779 8 10 7 2 0 0 0 0] [ 1 8 5 3258 7 2 0 0 0 0 0] [ 1 199 140 657 44 3 2 0 0 0 0] [ 1 4 18 85 473 3 2 0 0 0 0] [ 1 28 233 33 1853 3 2 0 0 0 0] [ 1 4 25 2415 68 3 2 0 0 0 0] [ 1 14 42 9 69 134 3 2 0 0 0] [ 1 16 262 6 3 2 0 0 0 0 0] [ 1 14 11 9 443 159 3 2 0 0 0] [ 1 21 165 919 8 3 2 0 0 0 0] [ 1 4 1250 1111 3 2 0 0 0 0 0] [ 1 56 185 13 201 3 2 0 0 0 0] [ 1 992 8 9 4415 3 2 0 0 0 0] [ 1 5 1360 596 265 3 2 0 0 0 0] [ 1 6 23 35 17 3 2 0 0 0 0] [ 1 4 135 1773 3 2 0 0 0 0 0] [ 1 5 825 9 578 3 2 0 0 0 0] [ 1 4 62 884 376 3 2 0 0 0 0] [ 1 30 12 456 31 837 3 2 0 0 0] [ 1 46 11 279 15 544 3 2 0 0 0] [ 1 71 8 31 168 7 2 0 0 0 0] [ 1 4 18 537 4 169 73 3 2 0 0] [ 1 25 6 2944 20 7 2 0 0 0 0] [ 1 4 29 2329 59 97 3 2 0 0 0] [ 1 4 29 66 493 3 2 0 0 0 0] [ 1 10 11 69 15 40 89 3 2 0 0] [ 1 14 87 12 72 486 3 2 0 0 0] [ 1 32 11 13 1108 7 2 0 0 0 0] [ 1 6 92 348 3 2 0 0 0 0 0] [ 1 441 16 25 149 3 2 0 0 0 0] [ 1 22 6 35 1856 7 2 0 0 0 0] [ 1 4 43 126 67 3 2 0 0 0 0] [ 1 5 26 624 50 3 2 0 0 0 0] [ 1 5 905 54 73 3 2 0 0 0 0] [ 1 22 6 103 63 383 7 2 0 0 0] [ 1 42 6 202 242 7 2 0 0 0 0] [ 1 27 11 9 421 2264 3 2 0 0 0] [ 1 271 6 35 9 104 7 2 0 0 0] [ 1 5 8 1046 3 2 0 0 0 0 0] [ 1 24 31 344 438 7 2 0 0 0 0] [ 1 32 271 5 47 7 2 0 0 0 0] [ 1 60 1206 6 7 2 0 0 0 0 0] [ 1 4 472 417 9 1227 3 2 0 0 0] [ 1 6 25 12 302 17 3 2 0 0 0] [ 1 4 25 12 64 197 10 3 2 0 0] [ 1 4 65 105 21 2271 3 2 0 0 0] [ 1 16 65 160 4279 3 2 0 0 0 0] [ 1 32 478 717 7 2 0 0 0 0 0] [ 1 755 496 3 2 0 0 0 0 0 0] [ 1 6 24 85 273 3 2 0 0 0 0] [ 1 5 872 319 3 2 0 0 0 0 0] [ 1 9 728 8 9 728 3 2 0 0 0] [ 1 5 411 1522 3 2 0 0 0 0 0] [ 1 28 42 10 3 2 0 0 0 0 0] [ 1 25 4 125 10 44 7 2 0 0 0] [ 1 28 23 649 3 2 0 0 0 0 0] [ 1 13 850 92 293 3 2 0 0 0 0] [ 1 4 30 12 510 43 3 2 0 0 0] [ 1 4 18 34 2363 3 2 0 0 0 0] [ 1 379 1020 3 2 0 0 0 0 0 0] [ 1 10 11 21 205 3 2 0 0 0 0] [ 1 19 8 31 385 3 2 0 0 0 0]], shape=(64, 11), dtype=int32)12345# 模型的定义部分embedding_units = 256 # 词向量.shape (embedding, )units = 1024 # 隐层神经元个数 某时间步隐层状态.shape (units, )input_vocab_size = len(input_tokenizer.word_index) + 1 # 词表用于embedding matrixoutput_vocab_size = len(output_tokenizer.word_index) + 1 # 同上 12345678910111213141516171819202122232425262728class Encoder(keras.Model): def __init__(self, vocab_size, embedding_units, encoding_units, batch_size): super(Encoder, self).__init__() # 调用父类构造函数 self.batch_size = batch_size # 创建实例变量，下同 self.encoding_units = encoding_units self.embedding = keras.layers.Embedding(vocab_size, embedding_units) self.gru = keras.layers.GRU(self.encoding_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform') def call(self, x, hidden): # hidden是初始化的隐含状态 x = self.embedding(x) output, state = self.gru(x, initial_state=hidden) return output, state def initialize_hidden_state(self): '''创建一个全是0的隐含状态，传给call函数''' return tf.zeros((self.batch_size, self.encoding_units))encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)sample_hidden = encoder.initialize_hidden_state()sample_output, sample_hidden = encoder(x, sample_hidden)print("sample_output_shape", sample_output.shape)print("sample_hidden_shape", sample_hidden.shape) sample_output_shape (64, 16, 1024) sample_hidden_shape (64, 1024)123456789101112131415161718192021222324252627class BahdananAttention(keras.Model): def __init__(self, units): super(BahdananAttention, self).__init__() self.W1 = keras.layers.Dense(units) self.W2 = keras.layers.Dense(units) self.V = keras.layers.Dense(1) def call(self, decoder_hidden, encoder_outputs): """decoder_hidden:decoder某一时间步的隐层状态，encoder_output:encoder每一时间步的输出""" # decoder_hidden.shape：(batch_size, units) # encoder_hidden.shape:(batch_size, length, units) # 广播机制 # 1. 后缘维度轴长相等 2. 其中1方长度为1 decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, axis=1) # before:(batch_size, length, units) # after:(batch_size, length, 1) score = self.V( tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis))) # shape:(batch_size, length, 1) attention_weights = tf.nn.softmax(score, axis=1) # context_vector.shape = (batch_size, length, units) context_vector = attention_weights * encoder_outputs # context_vector = (batch_size, units) context_vector = tf.reduce_sum(context_vector, axis=1) return context_vector, attention_weights 12345attention_model = BahdananAttention(units = 10)attention_results, attention_weights = attention_model(sample_hidden, sample_output)print("attention_results.shape:", attention_results.shape)print("attention_weights.shape:", attention_weights.shape) attention_results.shape: (64, 1024) attention_weights.shape: (64, 16, 1)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Decoder(keras.Model): def __init__(self, vocab_size, embedding_units, decoding_units, batch_size): super(Decoder, self).__init__() self.batch_size = batch_size self.decoding_units = decoding_units self.embedding = keras.layers.Embedding(vocab_size, embedding_units) self.gru = keras.layers.GRU(self.decoding_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform') self.fc = keras.layers.Dense(vocab_size) self.attention = BahdananAttention(self.decoding_units) def call(self, x, hidden, encoding_outputs): ''' x: decoder 当前步的输入 hidden: 上一步的隐层状态 encoding_outputs: 经过注意力向量加权求和后的上下文向量 ''' # context_vector.shape: (batch_size, units) context_vector, attention_weights = self.attention( hidden, encoding_outputs) # before embedding: x.shape :(batch_size, 1) # after embedding: x.shape: (batch_size, 1, embedding_units ) x = self.embedding(x) combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1) # output.shape: (batch_size, 1, decoding_units) # state.shape: (batch_size, decoding_units) output, state = self.gru(combined_x) # output.shape : (batch_size, decoding_units) output = tf.reshape(output, (-1, output.shape[2])) # output.shape: (batch_size, vocab_size) output = self.fc(output) return output, state, attention_weightsdecoder = Decoder(output_vocab_size, embedding_units, units, batch_size)outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)decoder_output, decoder_hidden, decoder_aw = outputsprint(decoder_output.shape)print(decoder_hidden.shape)print(decoder_aw.shape) (64, 4935) (64, 1024) (64, 16, 1)123456789101112131415optimizer = keras.optimizers.Adam()loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')# 去掉padding项所产生的误差后再进行聚合# 单步损失函数def loss_function(real, pred): mask = tf.math.logical_not(tf.math.equal(real, 0)) # padding 为0 非padding为1 loss_ = loss_object(real, pred) mask = tf.cast(mask, dtype=loss_.dtype) loss_ *= mask return tf.reduce_mean(loss_) 12345678910111213141516171819202122232425# 计算多步损失函数，并做梯度下降@tf.functiondef train_step(inp, targ, encoding_hidden): loss = 0 with tf.GradientTape() as tape: encoding_outputs, encoding_hidden = encoder(inp, encoding_hidden) decoding_hidden = encoding_hidden # eg:&lt;start&gt; I am here &lt;end&gt; # 1. &lt;start&gt; -&gt; I # 2. I -&gt; am # 3. am -&gt; here # 4. here -&gt; &lt;end&gt; for t in range(0, targ.shape[1] - 1): # 通过前一个单词预测后一个单词。 decoding_input = tf.expand_dims(targ[:, t], 1)# 切片之后是一个向量（batch, ），应扩展成（batch_size * 1）的矩阵 predictions, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_outputs) loss += loss_function(targ[:, t + 1], predictions) batch_loss = loss / int(targ.shape[0]) variables = encoder.trainable_variables + decoder.trainable_variables gradients = tape.gradient(loss, variables) optimizer.apply_gradients(zip(gradients, variables)) return batch_loss 12345678910111213141516171819202122epochs = 10steps_per_epoch = len(input_tensor) // batch_sizefor epoch in range(epochs): start = time.time() encoding_hidden = encoder.initialize_hidden_state() total_loss = 0 for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)): '''从训练集中取出steps_per_epoch个batch的数据''' batch_loss = train_step(inp, targ, encoding_hidden) total_loss += batch_loss if batch % 100 == 0: '''每100个batch打印一次数据：Batch从0索引，Epoch从1索引，batch_loss是平均到每条样本上的loss''' print('Epoch &#123;&#125; Batch &#123;&#125; Loss &#123;:.4f&#125;'.format( epoch + 1, batch, batch_loss.numpy())) '''每遍历一个epoch后打印一次信息：其中Loss保留小数点后四位，/steps_per_epoch后为每条样本的平均误差''' print('Epoch &#123;&#125; Loss &#123;:.4f&#125;'.format(epoch + 1, total_loss / steps_per_epoch)) print('Time take 1 epoch &#123;&#125; sec\n'.format(time.time() - start)) Epoch 1 Batch 0 Loss 0.8035 Epoch 1 Batch 100 Loss 0.3535 Epoch 1 Batch 200 Loss 0.3359 Epoch 1 Batch 300 Loss 0.2851 Epoch 1 Batch 400 Loss 0.2671 Epoch 1 Loss 0.3293 Time take 1 epoch 2116.8650002479553 sec Epoch 2 Batch 0 Loss 0.2466 Epoch 2 Batch 100 Loss 0.2326 Epoch 2 Batch 200 Loss 0.2236 Epoch 2 Batch 300 Loss 0.2260 Epoch 2 Batch 400 Loss 0.1522 Epoch 2 Loss 0.2067 Time take 1 epoch 2149.2449696063995 sec Epoch 3 Batch 0 Loss 0.1331 Epoch 3 Batch 100 Loss 0.1548 Epoch 3 Batch 200 Loss 0.1314 Epoch 3 Batch 300 Loss 0.1256 Epoch 3 Batch 400 Loss 0.0868 Epoch 3 Loss 0.1272 Time take 1 epoch 2136.0347578525543 sec Epoch 4 Batch 0 Loss 0.0693 Epoch 4 Batch 100 Loss 0.0904 Epoch 4 Batch 200 Loss 0.0876 Epoch 4 Batch 300 Loss 0.0791 Epoch 4 Batch 400 Loss 0.0469 Epoch 4 Loss 0.0776 Time take 1 epoch 2133.8691380023956 sec Epoch 5 Batch 0 Loss 0.0511 Epoch 5 Batch 100 Loss 0.0523 Epoch 5 Batch 200 Loss 0.0537 Epoch 5 Batch 300 Loss 0.0500 Epoch 5 Batch 400 Loss 0.0347 Epoch 5 Loss 0.0483 Time take 1 epoch 2115.8476724624634 sec Epoch 6 Batch 0 Loss 0.0240 Epoch 6 Batch 100 Loss 0.0340 Epoch 6 Batch 200 Loss 0.0424 Epoch 6 Batch 300 Loss 0.0272 Epoch 6 Batch 400 Loss 0.0157 Epoch 6 Loss 0.0319 Time take 1 epoch 2182.366710424423 sec Epoch 7 Batch 0 Loss 0.0208 Epoch 7 Batch 100 Loss 0.0224 Epoch 7 Batch 200 Loss 0.0275 Epoch 7 Batch 300 Loss 0.0247 Epoch 7 Batch 400 Loss 0.0153 Epoch 7 Loss 0.0224 Time take 1 epoch 2116.347582578659 sec Epoch 8 Batch 0 Loss 0.0180 Epoch 8 Batch 100 Loss 0.0161 Epoch 8 Batch 200 Loss 0.0209 Epoch 8 Batch 300 Loss 0.0178 Epoch 8 Batch 400 Loss 0.0154 Epoch 8 Loss 0.0170 Time take 1 epoch 2139.7361178398132 sec Epoch 9 Batch 0 Loss 0.0099 Epoch 9 Batch 100 Loss 0.0096 Epoch 9 Batch 200 Loss 0.0128 Epoch 9 Batch 300 Loss 0.0173 Epoch 9 Batch 400 Loss 0.0094 Epoch 9 Loss 0.0136 Time take 1 epoch 2131.8980412483215 sec Epoch 10 Batch 0 Loss 0.0096 Epoch 10 Batch 100 Loss 0.0076 Epoch 10 Batch 200 Loss 0.0135 Epoch 10 Batch 300 Loss 0.0100 Epoch 10 Batch 400 Loss 0.0089 Epoch 10 Loss 0.0123 Time take 1 epoch 2125.301104068756 sec12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758def evaluate(input_sentence): attention_matrix = np.zeros((max_length_output, max_length_input)) input_sentence = preprocess_sentence(input_sentence) inputs = [ input_tokenizer.word_index[token] for token in input_sentence.split(' ') ] inputs = keras.preprocessing.sequence.pad_sequences( [inputs], maxlen=max_length_input, padding='post') inputs = tf.convert_to_tensor(inputs) results = ' ' # encoding_hidden = encoder.initialize_hidden_state() encoding_hidden = tf.zeros((1, units)) encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden) decoding_hidden = encoding_hidden # decoding_Inputs.shape: (1, 1) decoding_input = tf.expand_dims([output_tokenizer.word_index['&lt;start&gt;']], 0) for t in range(max_length_output): predictions, decoding_hidden, attention_weights = decoder( decoding_input, decoding_hidden, encoding_outputs) # attention_weights.shape: (batch_size, inputs_length, 1) (1, 16, 1) attention_weights = tf.reshape(attention_weights, (-1, )) attention_matrix[t] = attention_weights.numpy() # predictions.shape: (batch_size, vocab_size) (1, 4935), 通过下标对第0个元素索引，拿到数组的形状为(4935, )的向量 # 通过tf.argmax获得元素最大值的索引，其实就是单词的索引 predicted_id = tf.argmax(predictions[0]).numpy() results += output_tokenizer.index_word[predicted_id] + " " if output_tokenizer.index_word[predicted_id] == '&lt;end&gt;': return results, input_sentence, attention_matrix decoding_input = tf.expand_dims([predicted_id], 0) return results, input_sentence, attention_matrixdef plot_attention(attention_matrix, input_sentence, predicted_sentence): fig = plt.figure(figsize=(10, 10)) ax = fig.add_subplot(1, 1, 1) ax.matshow(attention_matrix, cmap='viridis') font_dict = &#123;'fontsize': 14&#125; ax.set_xticklabels([' '] + input_sentence, fontdict=font_dict, rotation=90) ax.set_yticklabels([' '] + predicted_sentence, fontdict=font_dict) plt.show()def translate(input_sentence): results, input_sentence, attention_matrix = evaluate(input_sentence) print("Input: %s" % (input_sentence)) print("Predicted translation: %s" % (results)) # attention_matrix矩阵中不是所有位置都有元素，为可视化方便，需要注意力矩阵进行切片 attention_matrix = attention_matrix[:len(results.split(' ')), :len( input_sentence.split(' '))] plot_attention(attention_matrix, input_sentence.split(' '), results.split(' ')) 1translate(u'Hace mucho frío aquí.') Input: &lt;start&gt; hace mucho frio aqui . &lt;end&gt; Predicted translation: it s very cold here . &lt;end&gt; 1translate(u'Esta es mi vida.') Input: &lt;start&gt; esta es mi vida . &lt;end&gt; Predicted translation: this is my life . &lt;end&gt; 1translate(u'¿Todavía estás en casa?') Input: &lt;start&gt; ¿ todavia estas en casa ? &lt;end&gt; Predicted translation: are you still at home ? &lt;end&gt; 12]]></content>
      <tags>
        <tag>机器翻译</tag>
        <tag>Seq2Seq</tag>
        <tag>RNN</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas VS Excel——函数填充、计算列]]></title>
    <url>%2F2019%2F10%2F30%2FPandas-VS-Excel%E2%80%94%E2%80%94%E5%87%BD%E6%95%B0%E5%A1%AB%E5%85%85%E3%80%81%E8%AE%A1%E7%AE%97%E5%88%97%2F</url>
    <content type="text"><![CDATA[1import pandas as pd 导入数据帧1books = pd.read_excel('C:/Temp/Books.xlsx', index_col='ID') 第一种方法（遍历）books中Price列的第i个元素 = books中ListPrice列的第i个元素 * books中Discount列的第i个元素。适用于部分计算。 12for i in books.index: # 拿到每一行的索引即1-20 books['Price'].at[i] = books['ListPrice'].at[i] * books['Discount'].at[i] 第二种方法（以列为对象，操作列）在pandas中，使用函数填充时，操作对象就是整个列,适用于从头到尾的计算。 1books['Price'] = books['ListPrice']*books['Discount'] 书涨价的情况1books['ListPrice'] = books['ListPrice'] + 2 使用Series中的apply（apply中只写函数名即可，无需调用函数）123def add_2(x): # 定义一个函数加2 return x+2books['ListPrice'] = books['ListPrice'].apply(add_2) # 注意只使用函数名，后面不加括号，加括号表示调用]]></content>
      <tags>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[L1与L2正则化的区别]]></title>
    <url>%2F2019%2F10%2F29%2FL1%E4%B8%8EL2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;正则化的作用主要是防止过拟合。对代价函数添加正则化项可以限制模型的复杂程度，使得模型在复杂度和准确度上达到平衡。正则化可以看成是损失函数添加惩罚项。所谓惩罚就是指减小模型中的高阶项，但由于无法得知高阶项具体为哪些，所以需要对所有w参数进行惩罚。由于实际应用中，w的参数远多于b参数，所以约定俗成只对w参数进行惩罚，而不对b参数进行惩罚。]]></content>
      <categories>
        <category>ANN</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas VS Excel]]></title>
    <url>%2F2019%2F10%2F29%2FPandas-VS-Excel%2F</url>
    <content type="text"><![CDATA[行、列、单元格12345import pandas as pdL1 = [100, 200, 300]L2 = [&apos;x&apos;, &apos;y&apos;, &apos;z&apos;]s1 = pd.Series(L1, index=L2)print(s1) x 100 y 200 z 300 dtype: int64此时只是定义了一个序列，并没有定义行还是列。只有当添加到一个DataFrame时才是距离的行和列。 12345# 定义序列s1 = pd.Series([1, 2, 3], index=[1, 2, 3], name=&apos;A&apos;)s2 = pd.Series([10, 20, 30], index=[1, 2, 3], name=&apos;B&apos;)s3 = pd.Series([100, 200, 300], index=[1, 2, 3], name=&apos;C&apos;)s4 = pd.Series([100, 200, 300], index=[2, 3, 4], name=&apos;D&apos;) 12df1 = pd.DataFrame(&#123;s1.name:s1, s2.name:s2, s3.name:s3&#125;)print(df1) A B C 1 1 10 100 2 2 20 200 3 3 30 300以字典的方式定义的数据帧是按列排列的，这也是最标准的定义方式 12df2 = pd.DataFrame([s1, s2, s3])print(df2) 1 2 3 A 1 2 3 B 10 20 30 C 100 200 300以列表的方式定义的数据帧是按行排列的，注意区别 12df3 = pd.DataFrame(&#123;s1.name:s1, s2.name:s2, s4.name:s4&#125;)print(df3) A B D 1 1.0 10.0 NaN 2 2.0 20.0 100.0 3 3.0 30.0 200.0 4 NaN NaN 300.01234当序列的索引无法完全对其时：按索引进行排列，未对齐处用NaN（Not a numb）补充# 数据区域读取、填充数字、填充日期序列 import pandas as pd from datetime import date, timedelta def add_month(d, md): yd = md//12 m = d.month + md % 12 if m != 12: yd += m // 12 m = m % 12 return date(d.year + yd, m , d.day) # 将数据读入books;并且跳过前3行;使用C到F列;数据帧中ID列的数据类型设置为整型(如果直接设置为整型会报错，所以设置成字符串类型) books = pd.read_excel(&apos;C:/Temp/Book1.xlsx&apos;, skiprows=3, usecols=&apos;C:F&apos;, dtype={&apos;ID&apos;: str, &apos;inStore&apos;: str, &apos;Date&apos;: str}) start = date(2018, 1, 1) # 设置初始日期 print(books) # 打印数据帧books print(books[&apos;ID&apos;]) # 打印books中的ID列 print(type(books[&apos;ID&apos;])) # 打印ID列的属性 books[&apos;ID&apos;].at[0] = 100 # ID列中0索引处设置为100 print(books[&apos;ID&apos;]) for i in books.index: books[&apos;ID&apos;].at[i] = i + 1 books[&apos;inStore&apos;].at[i] = &apos;Yes&apos; if i%2 == 0 else &apos;No&apos; # 在Python中if...else...除了组成语句之外还会组成表达表达式。 books[&apos;Date&apos;].at[i] = add_month(start, i) books.set_index(&apos;ID&apos;, inplace=True) print(books) books.to_excel(&apos;C:/Temp/Output.xlsx&apos;)1## 运行结果 ID Name inStore Date 0 NaN Book_001 NaN NaN 1 NaN Book_002 NaN NaN 2 NaN Book_003 NaN NaN 3 NaN Book_004 NaN NaN 4 NaN Book_005 NaN NaN 5 NaN Book_006 NaN NaN 6 NaN Book_007 NaN NaN 7 NaN Book_008 NaN NaN 8 NaN Book_009 NaN NaN 9 NaN Book_010 NaN NaN 10 NaN Book_011 NaN NaN 11 NaN Book_012 NaN NaN 12 NaN Book_013 NaN NaN 13 NaN Book_014 NaN NaN 14 NaN Book_015 NaN NaN 15 NaN Book_016 NaN NaN 16 NaN Book_017 NaN NaN 17 NaN Book_018 NaN NaN 18 NaN Book_019 NaN NaN 19 NaN Book_020 NaN NaN 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 NaN 9 NaN 10 NaN 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 NaN 17 NaN 18 NaN 19 NaN Name: ID, dtype: object &lt;class &apos;pandas.core.series.Series&apos;&gt; 0 100 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 NaN 9 NaN 10 NaN 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 NaN 17 NaN 18 NaN 19 NaN Name: ID, dtype: object Name inStore Date ID 1 Book_001 Yes 2018-01-01 2 Book_002 No 2018-02-01 3 Book_003 Yes 2018-03-01 4 Book_004 No 2018-04-01 5 Book_005 Yes 2018-05-01 6 Book_006 No 2018-06-01 7 Book_007 Yes 2018-07-01 8 Book_008 No 2018-08-01 9 Book_009 Yes 2018-09-01 10 Book_010 No 2018-10-01 11 Book_011 Yes 2018-11-01 12 Book_012 No 2018-12-01 13 Book_013 Yes 2019-01-01 14 Book_014 No 2019-02-01 15 Book_015 Yes 2019-03-01 16 Book_016 No 2019-04-01 17 Book_017 Yes 2019-05-01 18 Book_018 No 2019-06-01 19 Book_019 Yes 2019-07-01 20 Book_020 No 2019-08-01]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络算法代码实现]]></title>
    <url>%2F2019%2F10%2F12%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[NeraulNetwork.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244import numpy as np# 双曲正切函数def tanh(x): return np.tanh(x)# 双曲正切函数导数def tanh_derivative(x): return 1 - np.tanh(x) * np.tanh(x)# sigmoid函数def sigmoid(x): return 1 / (1 + np.exp(-x))# sigmoid函数导数def sigmoid_derivative(x): return sigmoid(x) * (1 - sigmoid(x))# 神经网络实现类class NeuralNetwork(): # layers:神经网络 # activation:激励函数 # learning_rate 学习率 # loss_threshold:损失阀值 # epoch:最大训练次数 def __init__(self, layers=[], activation=&quot;sigmoid&quot;, learning_rate=0.1, epoch=1000, loss_threshold=0.01): if activation == &quot;sigmoid&quot;: self.activation = sigmoid self.activation_derivative = sigmoid_derivative elif activation == &quot;tanh&quot;: self.activation = tanh self.activation_derivative = tanh_derivative else: self.activation = sigmoid self.activation_derivative = sigmoid_derivative self.layers = layers self.init_weights(layers) self.init_bias(layers) self.init_nodes() self.init_errors() self.learning_rate = learning_rate self.epoch = epoch self.loss_threshold = loss_threshold # 校验二维数组 def valiad_two_array(self, data): if isinstance(data, list) and len(data) &gt; 0: if isinstance(data[0], list) == False or len(data[0]) == 0: raise RuntimeError(&quot;参数错误,请传一个不为空的二维数组&quot;) else: raise RuntimeError(&quot;参数错误,请传一个不为空的二维数组&quot;) # 校验一维数组 def valid_one_array(self, data): if isinstance(data, list) == False or len(data) == 0: raise RuntimeError(&quot;参数错误,请传入一个不为空的一维数组&quot;) # 初始化权重 def init_weights(self, layers): self.weights = [] for i in range(1, len(layers)): self.weights.append(np.random.random((layers[i - 1], layers[i]))) # 初始化偏向 def init_bias(self, layers): self.bias = [] for i in range(1, len(layers)): self.bias.append(np.random.random((layers[i], 1))) # 训练模型 def fit(self, data): self.valiad_two_array(data) self.counter = 0 for i in range(len(data)): self.training_data(data[i], i) # 预测数据 def predict(self, data): self.valiad_two_array(data) counter = 0 for one_data in data: self.forward_propagation(one_data) predict = self.nodes[len(self.layers) - 1] for i in range(len(predict)): predict[i] = self.handle_by_threshold(predict[i]) print(&quot;predict[&#123;&#125;] = &#123;&#125; &quot;.format(counter, predict)) counter += 1 # 根据阀值处理数据 def handle_by_threshold(self, data): if data &gt;= 0.5: return 1 else: return 0 # 一次训练流程 def training_data(self, one_data, number): self.loss = 1 one_training_counter = 0 while self.loss &gt; self.loss_threshold and one_training_counter &lt; self.epoch: self.counter += 1 one_training_counter += 1 self.forward_propagation(one_data) self.back_propagation_error(one_data) self.back_propagation_update_weights() self.back_propagation_update_bias() # print(&quot;总次数&#123;&#125;,第&#123;&#125;行数据,当前次数:&#123;&#125;,\n&#123;&#125;&quot;. # format(self.counter, number, one_training_counter, self.get_obj_info())) # 获取对象信息 def get_obj_info(self): info = &quot;\n\n weights: &quot; + str(self.weights) \ + &quot;\n\n bais: &quot; + str(self.bias) \ + &quot;\n\n nodes: &quot; + str(self.nodes) \ + &quot;\n\n errors: &quot; + str(self.errors) \ + &quot;\n\n loss: &quot; + str(self.loss) return info # 输出层错误计算 # out:经过激励函数计算后的结果 # predict:原始预测的结果 def calculate_out_layer_error(self, out, predict): return out * (1 - out) * (predict - out) # 隐藏层错误计算 # out:经过激励函数计算后的结果 # errors:下一层所有节点的损失合计 def calculate_hidden_layer_error(self, out, errors): return out * (1 - out) * errors # 前向传播,递归得到每一个节点的值 # one_row_data:一行数据 # counter: 计数器 def forward_propagation(self, one_row_data, counter=0): if counter == 0: input = self.get_input(one_row_data) self.input = input for i in range(len(self.input)): self.nodes[0][i] = self.input[i] counter += 1 if counter == len(self.layers): return current_nodes = self.nodes[counter] pre_nodes = self.nodes[counter - 1] for i in range(len(current_nodes)): current_value = 0 for j in range(len(pre_nodes)): pre_node = pre_nodes[j] pre_weights = self.weights[counter - 1][j][i] current_value += pre_node * pre_weights current_bias = self.bias[counter - 1][i][0] current_value = (current_value + current_bias)[0] current_node = self.activation(current_value) current_nodes[i] = current_node self.forward_propagation(one_row_data, counter + 1) # 得到特征值 def get_input(self, one_row_data): return one_row_data[:self.layers[0]] # 根据特征值真实结果 def get_out(self, one_row_data): return one_row_data[self.layers[0]:] # 后向传播,得到误差 def back_propagation_error(self, one_row_data, counter=-1): if counter == -1: # 第一次进入方法，初始化 counter = len(self.layers) - 1 out = self.get_out(one_row_data) self.out = out if counter == 0: # 遍历集合(第一层输入层不计算损失) return current_nodes = self.nodes[counter] if counter == len(self.layers) - 1: # 输出层损失计算 loss = 0 for i in range(len(current_nodes)): current_node = current_nodes[i] predict = self.out[i] error_value = self.calculate_out_layer_error(current_node, predict) self.errors[counter][i] = error_value loss += pow(predict - current_node, 2) self.loss = loss else: # 隐藏层损失计算 next_errors = self.errors[counter + 1] for i in range(len(current_nodes)): current_node = current_nodes[i] errors = 0 for j in range(len(next_errors)): error = next_errors[j] weight = self.weights[counter][i] errors += error * weight error_value = self.calculate_hidden_layer_error(current_node, errors) self.errors[counter][i] = error_value self.back_propagation_error(one_row_data, counter - 1) # 后向传播,更新权重 def back_propagation_update_weights(self): for i in reversed(range(len(self.layers) - 1)): current_nodes = self.nodes[i] errors = self.errors[i + 1] for j in range(len(current_nodes)): for m in range(len(errors)): error = errors[m] current_node = current_nodes[j] weight = self.weights[i][j][m] weight_delta = self.learning_rate * error * current_node update_weight = weight + weight_delta self.weights[i][j][m] = update_weight # 后向传播,更新偏向 def back_propagation_update_bias(self): for i in reversed(range(len(self.layers) - 1)): bias = self.bias[i] for j in range(len(bias)): error = self.errors[i + 1][j] bias_delta = self.learning_rate * error bias[j] += bias_delta # 设置权重 def set_weights(self, weights): self.weights = weights # 设置偏向 def set_bias(self, bias): self.bias = bias # 初始化所有节点（节点值设置为一个随机数) def init_nodes(self): self.nodes = [] for i in range(len(self.layers)): self.nodes.append(np.random.random((self.layers[i], 1))) # 初始化所有节点损失值(损失值设置为一个随机数) def init_errors(self): self.errors = [] for i in range(len(self.layers)): self.errors.append(np.random.random((self.layers[i], 1)))]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>ANN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cases of the Artificial Neural Networks]]></title>
    <url>%2F2019%2F10%2F08%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;人工神经网络（Artificial Neural Networks,简写ANN）是以一种模仿动物神经网络特征，进行分布式并行神经处理的算法数学模型。由大量的节点（或称神经元）组成。每个节点对应不同的输出函数，称为激活函数（activation function）,节点与节点之间对应不同的加权值，称之为权重。 前向传播 &emsp;&emsp;pass 反向传播&emsp;&emsp;反向传播（Backword Propagation）是“误差反向传播”的简称，是一种与梯度下降算法结合使用的，用来训练人工神经网络的重要算法。该算法通过计算损失函数对神经网络中各个权值的偏导数来最小化损失函数。是神经网络执行梯度下降算法的主要算法。该算法先前向计算出各个节点的状态以及输出值，然后再以反向计算损失函数对各个参数的偏导数。反向传播算法主要由激励传播和权值更新两个过程反复迭代，直到网络对输入的响应达到目标要求为止。本文将按照单一训练集介绍反向传播算法中重要公式的推导，后续扩展到m维训练集。&emsp;&emsp;单一训练集时符号说明如下： 参数矩阵的维度&emsp;&emsp;pass 非线性激活函数的对比分析&emsp;&emsp;pass]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Neural Network</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
</search>
