<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[L1与L2正则化的区别]]></title>
    <url>%2F2019%2F10%2F29%2FL1%E4%B8%8EL2%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;正则化的作用主要是防止过拟合。对代价函数添加正则化项可以限制模型的复杂程度，使得模型在复杂度和准确度上达到平衡。正则化可以看成是损失函数添加惩罚项。所谓惩罚就是指减小模型中的高阶项，但由于无法得知高阶项具体为哪些，所以需要对所有w参数进行惩罚。由于实际应用中，w的参数远多于b参数，所以约定俗成只对w参数进行惩罚，而不对b参数进行惩罚。]]></content>
      <categories>
        <category>ANN</category>
      </categories>
      <tags>
        <tag>正则化</tag>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas VS Excel]]></title>
    <url>%2F2019%2F10%2F29%2FPandas-VS-Excel%2F</url>
    <content type="text"><![CDATA[行、列、单元格12345import pandas as pdL1 = [100, 200, 300]L2 = [&apos;x&apos;, &apos;y&apos;, &apos;z&apos;]s1 = pd.Series(L1, index=L2)print(s1) x 100 y 200 z 300 dtype: int64此时只是定义了一个序列，并没有定义行还是列。只有当添加到一个DataFrame时才是距离的行和列。 12345# 定义序列s1 = pd.Series([1, 2, 3], index=[1, 2, 3], name=&apos;A&apos;)s2 = pd.Series([10, 20, 30], index=[1, 2, 3], name=&apos;B&apos;)s3 = pd.Series([100, 200, 300], index=[1, 2, 3], name=&apos;C&apos;)s4 = pd.Series([100, 200, 300], index=[2, 3, 4], name=&apos;D&apos;) 12df1 = pd.DataFrame(&#123;s1.name:s1, s2.name:s2, s3.name:s3&#125;)print(df1) A B C 1 1 10 100 2 2 20 200 3 3 30 300以字典的方式定义的数据帧是按列排列的，这也是最标准的定义方式 12df2 = pd.DataFrame([s1, s2, s3])print(df2) 1 2 3 A 1 2 3 B 10 20 30 C 100 200 300以列表的方式定义的数据帧是按行排列的，注意区别 12df3 = pd.DataFrame(&#123;s1.name:s1, s2.name:s2, s4.name:s4&#125;)print(df3) A B D 1 1.0 10.0 NaN 2 2.0 20.0 100.0 3 3.0 30.0 200.0 4 NaN NaN 300.01234当序列的索引无法完全对其时：按索引进行排列，未对齐处用NaN（Not a numb）补充# 数据区域读取、填充数字、填充日期序列 import pandas as pd from datetime import date, timedelta def add_month(d, md): yd = md//12 m = d.month + md % 12 if m != 12: yd += m // 12 m = m % 12 return date(d.year + yd, m , d.day) # 将数据读入books;并且跳过前3行;使用C到F列;数据帧中ID列的数据类型设置为整型(如果直接设置为整型会报错，所以设置成字符串类型) books = pd.read_excel(&apos;C:/Temp/Book1.xlsx&apos;, skiprows=3, usecols=&apos;C:F&apos;, dtype={&apos;ID&apos;: str, &apos;inStore&apos;: str, &apos;Date&apos;: str}) start = date(2018, 1, 1) # 设置初始日期 print(books) # 打印数据帧books print(books[&apos;ID&apos;]) # 打印books中的ID列 print(type(books[&apos;ID&apos;])) # 打印ID列的属性 books[&apos;ID&apos;].at[0] = 100 # ID列中0索引处设置为100 print(books[&apos;ID&apos;]) for i in books.index: books[&apos;ID&apos;].at[i] = i + 1 books[&apos;inStore&apos;].at[i] = &apos;Yes&apos; if i%2 == 0 else &apos;No&apos; # 在Python中if...else...除了组成语句之外还会组成表达表达式。 books[&apos;Date&apos;].at[i] = add_month(start, i) books.set_index(&apos;ID&apos;, inplace=True) print(books) books.to_excel(&apos;C:/Temp/Output.xlsx&apos;)1## 运行结果 ID Name inStore Date 0 NaN Book_001 NaN NaN 1 NaN Book_002 NaN NaN 2 NaN Book_003 NaN NaN 3 NaN Book_004 NaN NaN 4 NaN Book_005 NaN NaN 5 NaN Book_006 NaN NaN 6 NaN Book_007 NaN NaN 7 NaN Book_008 NaN NaN 8 NaN Book_009 NaN NaN 9 NaN Book_010 NaN NaN 10 NaN Book_011 NaN NaN 11 NaN Book_012 NaN NaN 12 NaN Book_013 NaN NaN 13 NaN Book_014 NaN NaN 14 NaN Book_015 NaN NaN 15 NaN Book_016 NaN NaN 16 NaN Book_017 NaN NaN 17 NaN Book_018 NaN NaN 18 NaN Book_019 NaN NaN 19 NaN Book_020 NaN NaN 0 NaN 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 NaN 9 NaN 10 NaN 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 NaN 17 NaN 18 NaN 19 NaN Name: ID, dtype: object &lt;class &apos;pandas.core.series.Series&apos;&gt; 0 100 1 NaN 2 NaN 3 NaN 4 NaN 5 NaN 6 NaN 7 NaN 8 NaN 9 NaN 10 NaN 11 NaN 12 NaN 13 NaN 14 NaN 15 NaN 16 NaN 17 NaN 18 NaN 19 NaN Name: ID, dtype: object Name inStore Date ID 1 Book_001 Yes 2018-01-01 2 Book_002 No 2018-02-01 3 Book_003 Yes 2018-03-01 4 Book_004 No 2018-04-01 5 Book_005 Yes 2018-05-01 6 Book_006 No 2018-06-01 7 Book_007 Yes 2018-07-01 8 Book_008 No 2018-08-01 9 Book_009 Yes 2018-09-01 10 Book_010 No 2018-10-01 11 Book_011 Yes 2018-11-01 12 Book_012 No 2018-12-01 13 Book_013 Yes 2019-01-01 14 Book_014 No 2019-02-01 15 Book_015 Yes 2019-03-01 16 Book_016 No 2019-04-01 17 Book_017 Yes 2019-05-01 18 Book_018 No 2019-06-01 19 Book_019 Yes 2019-07-01 20 Book_020 No 2019-08-01]]></content>
      <categories>
        <category>数据挖掘</category>
      </categories>
      <tags>
        <tag>pandas</tag>
        <tag>Excel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络算法代码实现]]></title>
    <url>%2F2019%2F10%2F12%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[NeraulNetwork.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244import numpy as np# 双曲正切函数def tanh(x): return np.tanh(x)# 双曲正切函数导数def tanh_derivative(x): return 1 - np.tanh(x) * np.tanh(x)# sigmoid函数def sigmoid(x): return 1 / (1 + np.exp(-x))# sigmoid函数导数def sigmoid_derivative(x): return sigmoid(x) * (1 - sigmoid(x))# 神经网络实现类class NeuralNetwork(): # layers:神经网络 # activation:激励函数 # learning_rate 学习率 # loss_threshold:损失阀值 # epoch:最大训练次数 def __init__(self, layers=[], activation=&quot;sigmoid&quot;, learning_rate=0.1, epoch=1000, loss_threshold=0.01): if activation == &quot;sigmoid&quot;: self.activation = sigmoid self.activation_derivative = sigmoid_derivative elif activation == &quot;tanh&quot;: self.activation = tanh self.activation_derivative = tanh_derivative else: self.activation = sigmoid self.activation_derivative = sigmoid_derivative self.layers = layers self.init_weights(layers) self.init_bias(layers) self.init_nodes() self.init_errors() self.learning_rate = learning_rate self.epoch = epoch self.loss_threshold = loss_threshold # 校验二维数组 def valiad_two_array(self, data): if isinstance(data, list) and len(data) &gt; 0: if isinstance(data[0], list) == False or len(data[0]) == 0: raise RuntimeError(&quot;参数错误,请传一个不为空的二维数组&quot;) else: raise RuntimeError(&quot;参数错误,请传一个不为空的二维数组&quot;) # 校验一维数组 def valid_one_array(self, data): if isinstance(data, list) == False or len(data) == 0: raise RuntimeError(&quot;参数错误,请传入一个不为空的一维数组&quot;) # 初始化权重 def init_weights(self, layers): self.weights = [] for i in range(1, len(layers)): self.weights.append(np.random.random((layers[i - 1], layers[i]))) # 初始化偏向 def init_bias(self, layers): self.bias = [] for i in range(1, len(layers)): self.bias.append(np.random.random((layers[i], 1))) # 训练模型 def fit(self, data): self.valiad_two_array(data) self.counter = 0 for i in range(len(data)): self.training_data(data[i], i) # 预测数据 def predict(self, data): self.valiad_two_array(data) counter = 0 for one_data in data: self.forward_propagation(one_data) predict = self.nodes[len(self.layers) - 1] for i in range(len(predict)): predict[i] = self.handle_by_threshold(predict[i]) print(&quot;predict[&#123;&#125;] = &#123;&#125; &quot;.format(counter, predict)) counter += 1 # 根据阀值处理数据 def handle_by_threshold(self, data): if data &gt;= 0.5: return 1 else: return 0 # 一次训练流程 def training_data(self, one_data, number): self.loss = 1 one_training_counter = 0 while self.loss &gt; self.loss_threshold and one_training_counter &lt; self.epoch: self.counter += 1 one_training_counter += 1 self.forward_propagation(one_data) self.back_propagation_error(one_data) self.back_propagation_update_weights() self.back_propagation_update_bias() # print(&quot;总次数&#123;&#125;,第&#123;&#125;行数据,当前次数:&#123;&#125;,\n&#123;&#125;&quot;. # format(self.counter, number, one_training_counter, self.get_obj_info())) # 获取对象信息 def get_obj_info(self): info = &quot;\n\n weights: &quot; + str(self.weights) \ + &quot;\n\n bais: &quot; + str(self.bias) \ + &quot;\n\n nodes: &quot; + str(self.nodes) \ + &quot;\n\n errors: &quot; + str(self.errors) \ + &quot;\n\n loss: &quot; + str(self.loss) return info # 输出层错误计算 # out:经过激励函数计算后的结果 # predict:原始预测的结果 def calculate_out_layer_error(self, out, predict): return out * (1 - out) * (predict - out) # 隐藏层错误计算 # out:经过激励函数计算后的结果 # errors:下一层所有节点的损失合计 def calculate_hidden_layer_error(self, out, errors): return out * (1 - out) * errors # 前向传播,递归得到每一个节点的值 # one_row_data:一行数据 # counter: 计数器 def forward_propagation(self, one_row_data, counter=0): if counter == 0: input = self.get_input(one_row_data) self.input = input for i in range(len(self.input)): self.nodes[0][i] = self.input[i] counter += 1 if counter == len(self.layers): return current_nodes = self.nodes[counter] pre_nodes = self.nodes[counter - 1] for i in range(len(current_nodes)): current_value = 0 for j in range(len(pre_nodes)): pre_node = pre_nodes[j] pre_weights = self.weights[counter - 1][j][i] current_value += pre_node * pre_weights current_bias = self.bias[counter - 1][i][0] current_value = (current_value + current_bias)[0] current_node = self.activation(current_value) current_nodes[i] = current_node self.forward_propagation(one_row_data, counter + 1) # 得到特征值 def get_input(self, one_row_data): return one_row_data[:self.layers[0]] # 根据特征值真实结果 def get_out(self, one_row_data): return one_row_data[self.layers[0]:] # 后向传播,得到误差 def back_propagation_error(self, one_row_data, counter=-1): if counter == -1: # 第一次进入方法，初始化 counter = len(self.layers) - 1 out = self.get_out(one_row_data) self.out = out if counter == 0: # 遍历集合(第一层输入层不计算损失) return current_nodes = self.nodes[counter] if counter == len(self.layers) - 1: # 输出层损失计算 loss = 0 for i in range(len(current_nodes)): current_node = current_nodes[i] predict = self.out[i] error_value = self.calculate_out_layer_error(current_node, predict) self.errors[counter][i] = error_value loss += pow(predict - current_node, 2) self.loss = loss else: # 隐藏层损失计算 next_errors = self.errors[counter + 1] for i in range(len(current_nodes)): current_node = current_nodes[i] errors = 0 for j in range(len(next_errors)): error = next_errors[j] weight = self.weights[counter][i] errors += error * weight error_value = self.calculate_hidden_layer_error(current_node, errors) self.errors[counter][i] = error_value self.back_propagation_error(one_row_data, counter - 1) # 后向传播,更新权重 def back_propagation_update_weights(self): for i in reversed(range(len(self.layers) - 1)): current_nodes = self.nodes[i] errors = self.errors[i + 1] for j in range(len(current_nodes)): for m in range(len(errors)): error = errors[m] current_node = current_nodes[j] weight = self.weights[i][j][m] weight_delta = self.learning_rate * error * current_node update_weight = weight + weight_delta self.weights[i][j][m] = update_weight # 后向传播,更新偏向 def back_propagation_update_bias(self): for i in reversed(range(len(self.layers) - 1)): bias = self.bias[i] for j in range(len(bias)): error = self.errors[i + 1][j] bias_delta = self.learning_rate * error bias[j] += bias_delta # 设置权重 def set_weights(self, weights): self.weights = weights # 设置偏向 def set_bias(self, bias): self.bias = bias # 初始化所有节点（节点值设置为一个随机数) def init_nodes(self): self.nodes = [] for i in range(len(self.layers)): self.nodes.append(np.random.random((self.layers[i], 1))) # 初始化所有节点损失值(损失值设置为一个随机数) def init_errors(self): self.errors = [] for i in range(len(self.layers)): self.errors.append(np.random.random((self.layers[i], 1)))]]></content>
      <categories>
        <category>Algorithm</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>ANN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cases of the Artificial Neural Networks]]></title>
    <url>%2F2019%2F10%2F08%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;人工神经网络（Artificial Neural Networks,简写ANN）是以一种模仿动物神经网络特征，进行分布式并行神经处理的算法数学模型。由大量的节点（或称神经元）组成。每个节点对应不同的输出函数，称为激活函数（activation function）,节点与节点之间对应不同的加权值，称之为权重。 前向传播 &emsp;&emsp;pass 反向传播&emsp;&emsp;反向传播（Backword Propagation）是“误差反向传播”的简称，是一种与梯度下降算法结合使用的，用来训练人工神经网络的重要算法。该算法通过计算损失函数对神经网络中各个权值的偏导数来最小化损失函数。是神经网络执行梯度下降算法的主要算法。该算法先前向计算出各个节点的状态以及输出值，然后再以反向计算损失函数对各个参数的偏导数。反向传播算法主要由激励传播和权值更新两个过程反复迭代，直到网络对输入的响应达到目标要求为止。本文将按照单一训练集介绍反向传播算法中重要公式的推导，后续扩展到m维训练集。&emsp;&emsp;单一训练集时符号说明如下： 参数矩阵的维度&emsp;&emsp;pass 非线性激活函数的对比分析&emsp;&emsp;pass]]></content>
      <categories>
        <category>Deep Learning</category>
        <category>Neural Network</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
        <tag>Algorithm</tag>
      </tags>
  </entry>
</search>
